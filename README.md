# Sign Language to Text Project

## Overview

This repository contains the code and resources for a Sign Language to Text project. The goal is to develop a system that translates sign language gestures into written text.

## Project Files

1. **app.ipynb:** Jupyter notebook for the main application.
2. **collect-data.ipynb:** Jupyter notebook for collecting data related to sign language gestures.
3. **image_processing.ipynb:** Jupyter notebook for image processing techniques used in the project.
4. **preprocessing.ipynb:** Jupyter notebook for data preprocessing steps.
5. **requirements_pip.txt:** File containing Python dependencies for the project, installable using pip.
6. **requirements_conda.txt:** File containing Python dependencies for the project, installable using conda.
7. **signs.png:** Image resource for sign language gestures.

## Getting Started

1. Open the Jupyter notebooks (app.ipynb, collect-data.ipynb, image_processing.ipynb, preprocessing.ipynb) in your Jupyter environment.
2. Run the cells sequentially to understand the project workflow.
3. Explore the image processing and data preprocessing techniques used in the project.

## Data Collection

- Use collect-data.ipynb to gather data related to sign language gestures.
- Ensure a diverse dataset to enhance the model's accuracy.

## Image Processing

- Refer to image_processing.ipynb for details on how images are processed before feeding them to the model.
- Understand the techniques applied to enhance feature extraction.

## Model Application

- app.ipynb focuses on the main application of the sign language to text translation.
- Run the cells to see the model in action and its output.

## Dataset

- https://drive.google.com/drive/folders/1p8T54lYZvFpSqGQe9hQHwq_LU-1eQAeg?usp=sharing

## Author
**Kusumanjli**
**https://www.linkedin.com/in/kusumanjli-khattar-166b881b1/**
